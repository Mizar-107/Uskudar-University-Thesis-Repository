{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e477a54f",
   "metadata": {},
   "source": [
    "# YOLO Model Comparison: Vanilla vs Fine-tuned Bosphorus Model\n",
    "\n",
    "This notebook compares the performance of vanilla YOLO models (YOLOv8n, YOLOv8s, YOLOv11n, YOLOv11s) against a fine-tuned Bosphorus ship detection model on a custom test dataset.\n",
    "\n",
    "**Metrics Evaluated:**\n",
    "- Detection count vs Ground Truth\n",
    "- Intersection over Union (IoU)\n",
    "- Average confidence score\n",
    "- Inference time (ms)\n",
    "\n",
    "**Output:** Comparison metrics (CSV), visual detection grids, and performance charts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df2d6d4",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d056b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "DATASET_PATH = \"/content/drive/MyDrive/Uskudar Uni Master/YOLO Model klasor/Test Dataset\"\n",
    "FINETUNED_MODEL_PATH = \"/content/drive/MyDrive/Uskudar Uni Master/YOLO Model klasor/runs/bosphorus_yolo11s_10884/weights/best.pt\"\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/Uskudar Uni Master/YOLO Model klasor/Comparisons\"\n",
    "\n",
    "# Test paths (images and labels directly under Test Dataset folder)\n",
    "TEST_IMAGES_PATH = os.path.join(DATASET_PATH, \"images\")\n",
    "TEST_LABELS_PATH = os.path.join(DATASET_PATH, \"labels\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Verify paths\n",
    "print(\"Path Verification:\")\n",
    "print(f\"  Dataset: {'OK' if os.path.exists(DATASET_PATH) else 'MISSING'}\")\n",
    "print(f\"  Fine-tuned model: {'OK' if os.path.exists(FINETUNED_MODEL_PATH) else 'MISSING'}\")\n",
    "print(f\"  Test images: {'OK' if os.path.exists(TEST_IMAGES_PATH) else 'MISSING'}\")\n",
    "print(f\"  Test labels: {'OK' if os.path.exists(TEST_LABELS_PATH) else 'MISSING'}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bebc55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ultralytics\n",
    "!pip install -U ultralytics\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import ultralytics\n",
    "print(f\"Ultralytics version: {ultralytics.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea6008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"No GPU detected - go to Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc574c9",
   "metadata": {},
   "source": [
    "## Step 2: Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc4554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ============================================================\n",
    "# VANILLA MODELS TO COMPARE (add/remove as needed)\n",
    "# ============================================================\n",
    "VANILLA_MODELS = {\n",
    "    'YOLOv8n': 'yolov8n.pt',   # Nano - fastest, smallest\n",
    "    'YOLOv8s': 'yolov8s.pt',   # Small\n",
    "    'YOLOv11n': 'yolo11n.pt',  # Nano v11\n",
    "    'YOLOv11s': 'yolo11s.pt',  # Small v11 (same arch as fine-tuned)\n",
    "}\n",
    "# ============================================================\n",
    "\n",
    "# Load vanilla models\n",
    "vanilla_models = {}\n",
    "for name, weights in VANILLA_MODELS.items():\n",
    "    print(f\"Loading {name}...\", end=\" \")\n",
    "    vanilla_models[name] = YOLO(weights)\n",
    "    print(f\"OK ({len(vanilla_models[name].names)} COCO classes)\")\n",
    "\n",
    "# Fine-tuned Bosphorus model\n",
    "print(f\"\\nLoading Fine-tuned Bosphorus model...\", end=\" \")\n",
    "finetuned_model = YOLO(FINETUNED_MODEL_PATH)\n",
    "print(f\"OK\")\n",
    "print(f\"  Classes: {finetuned_model.names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecd2a20",
   "metadata": {},
   "source": [
    "## Step 3: Select Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d282343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select random test images\n",
    "import random\n",
    "import os\n",
    "\n",
    "NUM_IMAGES = 8  # Select 5-10 images\n",
    "\n",
    "# Get all test images\n",
    "all_test_images = [f for f in os.listdir(TEST_IMAGES_PATH) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "print(f\"Total test images: {len(all_test_images)}\")\n",
    "\n",
    "# Random selection\n",
    "random.seed(42)  # For reproducibility\n",
    "selected_images = random.sample(all_test_images, min(NUM_IMAGES, len(all_test_images)))\n",
    "\n",
    "print(f\"\\nSelected {len(selected_images)} images for comparison:\")\n",
    "for img in selected_images:\n",
    "    print(f\"  - {img}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27725062",
   "metadata": {},
   "source": [
    "## Step 4: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e514b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "def parse_yolo_label(label_path, img_width, img_height):\n",
    "    \"\"\"Parse YOLO label file to absolute coordinates.\"\"\"\n",
    "    boxes = []\n",
    "    if not os.path.exists(label_path):\n",
    "        return boxes\n",
    "    \n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 5:\n",
    "                x_center = float(parts[1]) * img_width\n",
    "                y_center = float(parts[2]) * img_height\n",
    "                width = float(parts[3]) * img_width\n",
    "                height = float(parts[4]) * img_height\n",
    "                \n",
    "                x1 = x_center - width / 2\n",
    "                y1 = y_center - height / 2\n",
    "                x2 = x_center + width / 2\n",
    "                y2 = y_center + height / 2\n",
    "                boxes.append([x1, y1, x2, y2])\n",
    "    return boxes\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate IoU between two boxes.\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def compute_avg_iou(pred_boxes, gt_boxes):\n",
    "    \"\"\"Compute average best IoU for predictions against ground truth (only matched boxes).\"\"\"\n",
    "    if not pred_boxes or not gt_boxes:\n",
    "        return 0.0\n",
    "    best_ious = [max(calculate_iou(pred, gt) for gt in gt_boxes) for pred in pred_boxes]\n",
    "    # Only average IoUs > 0 (matched predictions)\n",
    "    matched_ious = [iou for iou in best_ious if iou > 0]\n",
    "    return np.mean(matched_ious) if matched_ious else 0.0\n",
    "\n",
    "def compute_detection_metrics(pred_boxes, gt_boxes, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and matched IoU using Hungarian matching.\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'precision', 'recall', 'avg_matched_iou', 'true_positives', 'false_positives', 'false_negatives'\n",
    "    \"\"\"\n",
    "    if not pred_boxes and not gt_boxes:\n",
    "        return {'precision': 1.0, 'recall': 1.0, 'avg_matched_iou': 1.0, 'tp': 0, 'fp': 0, 'fn': 0}\n",
    "    if not pred_boxes:\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'avg_matched_iou': 0.0, 'tp': 0, 'fp': 0, 'fn': len(gt_boxes)}\n",
    "    if not gt_boxes:\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'avg_matched_iou': 0.0, 'tp': 0, 'fp': len(pred_boxes), 'fn': 0}\n",
    "    \n",
    "    # Greedy matching: for each prediction, find best GT match\n",
    "    gt_matched = [False] * len(gt_boxes)\n",
    "    matched_ious = []\n",
    "    tp = 0\n",
    "    \n",
    "    for pred in pred_boxes:\n",
    "        best_iou = 0\n",
    "        best_gt_idx = -1\n",
    "        for gt_idx, gt in enumerate(gt_boxes):\n",
    "            if gt_matched[gt_idx]:\n",
    "                continue\n",
    "            iou = calculate_iou(pred, gt)\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_gt_idx = gt_idx\n",
    "        \n",
    "        if best_iou >= iou_threshold and best_gt_idx >= 0:\n",
    "            gt_matched[best_gt_idx] = True\n",
    "            matched_ious.append(best_iou)\n",
    "            tp += 1\n",
    "    \n",
    "    fp = len(pred_boxes) - tp\n",
    "    fn = len(gt_boxes) - tp\n",
    "    \n",
    "    precision = tp / len(pred_boxes) if pred_boxes else 0.0\n",
    "    recall = tp / len(gt_boxes) if gt_boxes else 0.0\n",
    "    avg_matched_iou = np.mean(matched_ious) if matched_ious else 0.0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall, \n",
    "        'avg_matched_iou': avg_matched_iou,\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn\n",
    "    }\n",
    "\n",
    "def run_inference(model, image_path, imgsz, conf, classes=None):\n",
    "    \"\"\"\n",
    "    Run inference with specified parameters.\n",
    "    \n",
    "    Args:\n",
    "        model: YOLO model\n",
    "        image_path: Path to image\n",
    "        imgsz: Image size for inference\n",
    "        conf: Confidence threshold\n",
    "        classes: List of class indices to filter (None = all classes)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    results = model.predict(source=image_path, conf=conf, imgsz=imgsz, classes=classes, verbose=False)\n",
    "    inference_time = (time.time() - start_time) * 1000\n",
    "    \n",
    "    boxes = [box.xyxy[0].cpu().numpy().tolist() for r in results for box in r.boxes]\n",
    "    confidences = [float(box.conf[0]) for r in results for box in r.boxes]\n",
    "    \n",
    "    return {\n",
    "        'boxes': boxes,\n",
    "        'confidences': confidences,\n",
    "        'num_detections': len(boxes),\n",
    "        'avg_confidence': np.mean(confidences) if confidences else 0.0,\n",
    "        'inference_time_ms': inference_time,\n",
    "        'results': results\n",
    "    }\n",
    "\n",
    "print(\"Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c15a09d",
   "metadata": {},
   "source": [
    "## Step 5: Run Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a1218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# MODEL PARAMETERS\n",
    "# ============================================================\n",
    "# Vanilla models: DEFAULT/MINIMAL parameters (NO optimization)\n",
    "VANILLA_IMGSZ = 640      # Default YOLO size\n",
    "VANILLA_CONF = 0.25\n",
    "VANILLA_CLASSES = None   # No class filtering - detect everything\n",
    "\n",
    "# Fine-tuned: OPTIMAL parameters (as trained)\n",
    "FINETUNED_IMGSZ = 1088   # Optimal for 1920x1080 images\n",
    "FINETUNED_CONF = 0.25\n",
    "FINETUNED_CLASSES = None\n",
    "# ============================================================\n",
    "\n",
    "# Store results: {model_name: [per_image_results]}\n",
    "all_results = {name: [] for name in vanilla_models.keys()}\n",
    "all_results['Fine-tuned'] = []\n",
    "\n",
    "IOU_THRESHOLD = 0.5  # Standard threshold for matching\n",
    "\n",
    "print(\"Running comparison...\")\n",
    "print(f\"  Vanilla models: imgsz={VANILLA_IMGSZ}, conf={VANILLA_CONF}\")\n",
    "print(f\"  Fine-tuned:     imgsz={FINETUNED_IMGSZ}, conf={FINETUNED_CONF}\")\n",
    "print(f\"  IoU threshold:  {IOU_THRESHOLD}\")\n",
    "print()\n",
    "\n",
    "for img_idx, img_name in enumerate(selected_images):\n",
    "    img_path = os.path.join(TEST_IMAGES_PATH, img_name)\n",
    "    label_path = os.path.join(TEST_LABELS_PATH, os.path.splitext(img_name)[0] + '.txt')\n",
    "    \n",
    "    img = cv2.imread(img_path)\n",
    "    img_height, img_width = img.shape[:2]\n",
    "    gt_boxes = parse_yolo_label(label_path, img_width, img_height)\n",
    "    \n",
    "    print(f\"[{img_idx+1}/{len(selected_images)}] {img_name} (GT: {len(gt_boxes)} ships)\")\n",
    "    \n",
    "    # Run all vanilla models\n",
    "    for name, model in vanilla_models.items():\n",
    "        results = run_inference(model, img_path, VANILLA_IMGSZ, VANILLA_CONF, VANILLA_CLASSES)\n",
    "        metrics = compute_detection_metrics(results['boxes'], gt_boxes, IOU_THRESHOLD)\n",
    "        all_results[name].append({\n",
    "            'image': img_name,\n",
    "            'gt_count': len(gt_boxes),\n",
    "            'detections': results['num_detections'],\n",
    "            'avg_conf': results['avg_confidence'],\n",
    "            'precision': metrics['precision'],\n",
    "            'recall': metrics['recall'],\n",
    "            'avg_matched_iou': metrics['avg_matched_iou'],\n",
    "            'tp': metrics['tp'],\n",
    "            'fp': metrics['fp'],\n",
    "            'fn': metrics['fn'],\n",
    "            'time_ms': results['inference_time_ms'],\n",
    "            'raw': results\n",
    "        })\n",
    "        print(f\"    {name}: {results['num_detections']} det, P={metrics['precision']:.2f}, R={metrics['recall']:.2f}, IoU={metrics['avg_matched_iou']:.2f}, {results['inference_time_ms']:.0f}ms\")\n",
    "    \n",
    "    # Run fine-tuned model\n",
    "    results = run_inference(finetuned_model, img_path, FINETUNED_IMGSZ, FINETUNED_CONF, FINETUNED_CLASSES)\n",
    "    metrics = compute_detection_metrics(results['boxes'], gt_boxes, IOU_THRESHOLD)\n",
    "    all_results['Fine-tuned'].append({\n",
    "        'image': img_name,\n",
    "        'gt_count': len(gt_boxes),\n",
    "        'detections': results['num_detections'],\n",
    "        'avg_conf': results['avg_confidence'],\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall'],\n",
    "        'avg_matched_iou': metrics['avg_matched_iou'],\n",
    "        'tp': metrics['tp'],\n",
    "        'fp': metrics['fp'],\n",
    "        'fn': metrics['fn'],\n",
    "        'time_ms': results['inference_time_ms'],\n",
    "        'raw': results\n",
    "    })\n",
    "    print(f\"    Fine-tuned: {results['num_detections']} det, P={metrics['precision']:.2f}, R={metrics['recall']:.2f}, IoU={metrics['avg_matched_iou']:.2f}, {results['inference_time_ms']:.0f}ms\")\n",
    "\n",
    "print(\"\\nComparison complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75221fde",
   "metadata": {},
   "source": [
    "## Step 6: Generate Metrics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14d01b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics for all models\n",
    "model_names = list(all_results.keys())\n",
    "gt_total = sum(r['gt_count'] for r in all_results['Fine-tuned'])\n",
    "\n",
    "summary_stats = {}\n",
    "for name in model_names:\n",
    "    results = all_results[name]\n",
    "    total_det = sum(r['detections'] for r in results)\n",
    "    total_tp = sum(r['tp'] for r in results)\n",
    "    total_fp = sum(r['fp'] for r in results)\n",
    "    total_fn = sum(r['fn'] for r in results)\n",
    "    \n",
    "    # Calculate overall precision and recall\n",
    "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
    "    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    # Average matched IoU (only for true positives)\n",
    "    matched_ious = [r['avg_matched_iou'] for r in results if r['tp'] > 0]\n",
    "    avg_matched_iou = np.mean(matched_ious) if matched_ious else 0.0\n",
    "    \n",
    "    avg_conf = np.mean([r['avg_conf'] for r in results if r['avg_conf'] > 0]) if any(r['avg_conf'] > 0 for r in results) else 0\n",
    "    avg_time = np.mean([r['time_ms'] for r in results])\n",
    "    \n",
    "    summary_stats[name] = {\n",
    "        'Total Detections': total_det,\n",
    "        'TP': total_tp,\n",
    "        'FP': total_fp,\n",
    "        'FN': total_fn,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Avg Matched IoU': avg_matched_iou,\n",
    "        'Avg Confidence': avg_conf,\n",
    "        'Avg Time (ms)': avg_time\n",
    "    }\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame(summary_stats).T\n",
    "summary_df['Ground Truth'] = gt_total\n",
    "summary_df = summary_df[['Ground Truth', 'Total Detections', 'TP', 'FP', 'FN', 'Precision', 'Recall', 'F1 Score', 'Avg Matched IoU', 'Avg Confidence', 'Avg Time (ms)']]\n",
    "\n",
    "# Format for display\n",
    "display_df = summary_df.copy()\n",
    "display_df['Precision'] = display_df['Precision'].apply(lambda x: f\"{x:.2%}\")\n",
    "\n",
    "display_df['Recall'] = display_df['Recall'].apply(lambda x: f\"{x:.2%}\")print(f\"  Fastest:           {summary_df['Avg Time (ms)'].idxmin()} ({summary_df['Avg Time (ms)'].min():.1f}ms)\")\n",
    "\n",
    "display_df['F1 Score'] = display_df['F1 Score'].apply(lambda x: f\"{x:.2%}\")print(f\"  Best Confidence:   {summary_df['Avg Confidence'].idxmax()} ({summary_df['Avg Confidence'].max():.2%})\")\n",
    "\n",
    "display_df['Avg Matched IoU'] = display_df['Avg Matched IoU'].apply(lambda x: f\"{x:.3f}\")print(f\"  Best Matched IoU:  {summary_df['Avg Matched IoU'].idxmax()} ({summary_df['Avg Matched IoU'].max():.3f})\")\n",
    "\n",
    "display_df['Avg Confidence'] = display_df['Avg Confidence'].apply(lambda x: f\"{x:.2%}\")print(f\"  Best Recall:       {summary_df['Recall'].idxmax()} ({summary_df['Recall'].max():.2%})\")\n",
    "\n",
    "display_df['Avg Time (ms)'] = display_df['Avg Time (ms)'].apply(lambda x: f\"{x:.1f}\")print(f\"  Best Precision:    {summary_df['Precision'].idxmax()} ({summary_df['Precision'].max():.2%})\")\n",
    "\n",
    "print(f\"  Best F1 Score:     {summary_df['F1 Score'].idxmax()} ({summary_df['F1 Score'].max():.2%})\")\n",
    "\n",
    "print(\"=\" * 120)print(\"=\" * 120)\n",
    "\n",
    "print(f\"SUMMARY: ALL MODELS COMPARISON (IoU threshold = {IOU_THRESHOLD})\")print(\"BEST MODEL PER METRIC:\")\n",
    "\n",
    "print(\"=\" * 120)print(\"\\n\" + \"=\" * 120)\n",
    "\n",
    "display(display_df)# Find best model for each metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0fd1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics to CSV\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Per-image results for all models\n",
    "rows = []\n",
    "for img_idx in range(len(selected_images)):\n",
    "    row = {'image': all_results['Fine-tuned'][img_idx]['image'], \n",
    "           'gt_count': all_results['Fine-tuned'][img_idx]['gt_count']}\n",
    "    for name in model_names:\n",
    "        r = all_results[name][img_idx]\n",
    "        row[f'{name}_detections'] = r['detections']\n",
    "        row[f'{name}_tp'] = r['tp']\n",
    "        row[f'{name}_fp'] = r['fp']\n",
    "        row[f'{name}_fn'] = r['fn']\n",
    "        row[f'{name}_precision'] = r['precision']\n",
    "        row[f'{name}_recall'] = r['recall']\n",
    "        row[f'{name}_matched_iou'] = r['avg_matched_iou']\n",
    "        row[f'{name}_conf'] = r['avg_conf']\n",
    "        row[f'{name}_time_ms'] = r['time_ms']\n",
    "    rows.append(row)\n",
    "\n",
    "print(f\"Metrics saved to: {csv_path}\")\n",
    "\n",
    "csv_df = pd.DataFrame(rows)csv_df.to_csv(csv_path, index=False)\n",
    "csv_path = os.path.join(OUTPUT_DIR, f\"comparison_metrics_{timestamp}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a73fef",
   "metadata": {},
   "source": [
    "## Step 7: Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958d5dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def create_comparison_grid(all_results, model_names, output_path, max_images=4):\n",
    "    \"\"\"Create comparison grid for all models.\"\"\"\n",
    "    n_images = min(len(all_results['Fine-tuned']), max_images)\n",
    "    n_models = len(model_names)\n",
    "    \n",
    "    fig = plt.figure(figsize=(5 * n_models, 5 * n_images))\n",
    "    gs = GridSpec(n_images, n_models, figure=fig, hspace=0.3, wspace=0.1)\n",
    "    \n",
    "    for img_idx in range(n_images):\n",
    "        for model_idx, name in enumerate(model_names):\n",
    "            result = all_results[name][img_idx]\n",
    "            annotated = cv2.cvtColor(result['raw']['results'][0].plot(), cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            ax = fig.add_subplot(gs[img_idx, model_idx])\n",
    "            ax.imshow(annotated)\n",
    "            \n",
    "            imgsz = FINETUNED_IMGSZ if name == 'Fine-tuned' else VANILLA_IMGSZ\n",
    "            ax.set_title(f\"{name} (imgsz={imgsz})\\n{result['detections']} det | IoU: {result['iou']:.2f}\", \n",
    "                        fontsize=9, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "            \n",
    "            if model_idx == 0:\n",
    "                ax.text(-0.02, 0.5, f\"{result['image'][:20]}...\\n(GT: {result['gt_count']})\", \n",
    "                       transform=ax.transAxes, fontsize=8, va='center', ha='right', fontweight='bold')\n",
    "    \n",
    "    fig.suptitle('Model Comparison: Vanilla Models (Default Params) vs Fine-tuned (Optimal Params)', \n",
    "                 fontsize=12, fontweight='bold', y=1.02)\n",
    "    \n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    print(f\"\\nComparison grid saved to: {output_path}\")\n",
    "\n",
    "grid_path = os.path.join(OUTPUT_DIR, f\"comparison_grid_{timestamp}.png\")\n",
    "create_comparison_grid(all_results, model_names, grid_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736f9527",
   "metadata": {},
   "source": [
    "## Step 8: Speed Comparison Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb32e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Colors for each model\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(model_names)))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Inference Time Comparison (bar chart)\n",
    "ax1 = axes[0]\n",
    "times = [summary_stats[name]['Avg Time (ms)'] for name in model_names]\n",
    "bars = ax1.bar(model_names, times, color=colors, edgecolor='black', linewidth=1)\n",
    "ax1.set_ylabel('Avg Inference Time (ms)', fontsize=11)\n",
    "ax1.set_title('Inference Speed', fontsize=12, fontweight='bold')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "for bar, t in zip(bars, times):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "             f'{t:.0f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# 2. Average IoU Comparison (bar chart)\n",
    "ax2 = axes[1]\n",
    "ious = [summary_stats[name]['Avg IoU'] for name in model_names]\n",
    "bars = ax2.bar(model_names, ious, color=colors, edgecolor='black', linewidth=1)\n",
    "ax2.set_ylabel('Average IoU', fontsize=11)\n",
    "ax2.set_title('Detection Quality (IoU)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylim(0, 1.0)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "for bar, iou in zip(bars, ious):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{iou:.2f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# 3. Detection Count vs Ground Truth\n",
    "ax3 = axes[2]\n",
    "x = np.arange(len(selected_images))\n",
    "width = 0.8 / len(model_names)\n",
    "\n",
    "for i, name in enumerate(model_names):\n",
    "    dets = [all_results[name][j]['detections'] for j in range(len(selected_images))]\n",
    "    ax3.bar(x + i * width - 0.4 + width/2, dets, width, label=name, color=colors[i], alpha=0.8)\n",
    "\n",
    "gt_counts = [all_results['Fine-tuned'][j]['gt_count'] for j in range(len(selected_images))]\n",
    "ax3.plot(x, gt_counts, 'ko-', linewidth=2, markersize=6, label='Ground Truth')\n",
    "\n",
    "ax3.set_ylabel('Detection Count', fontsize=11)\n",
    "ax3.set_xlabel('Image', fontsize=11)\n",
    "ax3.set_title('Detections per Image', fontsize=12, fontweight='bold')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels([f'{i+1}' for i in range(len(selected_images))])\n",
    "ax3.legend(loc='upper right', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "speed_chart_path = os.path.join(OUTPUT_DIR, f\"comparison_charts_{timestamp}.png\")\n",
    "plt.savefig(speed_chart_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "print(f\"Charts saved to: {speed_chart_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea6f0fc",
   "metadata": {},
   "source": [
    "## Step 9: Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9209ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"=\" * 90)\n",
    "print(\"MULTI-MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "print(\"\\nPARAMETER CONFIGURATION:\")\n",
    "print(f\"  Vanilla models:       imgsz={VANILLA_IMGSZ} (default), conf={VANILLA_CONF}, classes=all\")\n",
    "print(f\"  Fine-tuned Bosphorus: imgsz={FINETUNED_IMGSZ} (optimal), conf={FINETUNED_CONF}, classes=all\")\n",
    "print(f\"\\nTest Images: {len(selected_images)} | Ground Truth Ships: {gt_total}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 90)\n",
    "print(f\"{'Model':<15} {'Detections':<12} {'Avg IoU':<10} {'Avg Conf':<12} {'Avg Time':<12} {'Notes'}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for name in model_names:\n",
    "    s = summary_stats[name]\n",
    "    notes = \"â­ BEST IoU\" if name == summary_df['Avg IoU'].idxmax() else \"\"\n",
    "    notes += \" ðŸš€ FASTEST\" if name == summary_df['Avg Time (ms)'].idxmin() else \"\"\n",
    "    print(f\"{name:<15} {s['Total Detections']:<12} {s['Avg IoU']:<10.3f} {s['Avg Confidence']:<12.2%} {s['Avg Time (ms)']:<12.1f} {notes}\")\n",
    "\n",
    "print(\"-\" * 90)\n",
    "print(f\"{'Ground Truth':<15} {gt_total:<12}\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "print(\"\\nOUTPUT FILES:\")\n",
    "print(f\"  - Metrics: {csv_path}\")\n",
    "print(f\"  - Visual:  {grid_path}\")\n",
    "print(f\"  - Charts:  {speed_chart_path}\")\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c743eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all output files exist\n",
    "print(\"\\nOutput files verification:\")\n",
    "for filepath in [csv_path, grid_path, speed_chart_path]:\n",
    "    if os.path.exists(filepath):\n",
    "        size_kb = os.path.getsize(filepath) / 1024\n",
    "        print(f\"  âœ“ {os.path.basename(filepath)} ({size_kb:.1f} KB)\")\n",
    "    else:\n",
    "        print(f\"  âœ— {os.path.basename(filepath)} - MISSING\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
