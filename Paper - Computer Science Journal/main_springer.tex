% Springer Nature SN Computer Science Submission
% Template: sn-jnl.cls (Version 3.1, December 2024)
% Reference Style: sn-mathphys-num (Numbered citations for Computer Science)

\documentclass[pdflatex,sn-mathphys-num,Numbered]{sn-jnl}

% Additional packages
\usepackage{graphicx}
\graphicspath{{./figures/}}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}

% Custom column types for tables
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}

\begin{document}

\title[Fine-tuned YOLO11 for Maritime Vessel Detection]{Fine-tuned YOLO11 for Maritime Vessel Detection in the Bosphorus Strait: A Comparative Study}

%%=============================================================================
%% Authors and Affiliations
%%=============================================================================

\author*[1]{\fnm{Recep Ertu\u{g}rul} \sur{Ek\c{s}i}}\email{recepertugrul.eksi@st.uskudar.edu.tr}

\author[1]{\fnm{Rowanda D.} \sur{Ahmed}}\email{rowanda.ahmed@uskudar.edu.tr}

\affil*[1]{
    \orgdiv{Department of Computer Engineering},
    \orgname{\"{U}sk\"{u}dar University},
    \orgaddress{
        \city{Istanbul},
        \country{Turkey}
    }
}

%%=============================================================================
%% Abstract
%%=============================================================================

\abstract{
Maritime vessel detection in the Bosphorus Strait is critical for traffic management, safety, and environmental monitoring. As one of the world's busiest waterways, the strait handles over 40,000 vessels annually, necessitating robust automated surveillance systems. However, general-purpose object detection models trained on standard datasets struggle with domain-specific maritime imagery due to unique lighting conditions, vessel types, and background complexity. This paper presents a fine-tuned YOLO11s model specifically trained on Bosphorus maritime imagery and provides a comprehensive comparison against vanilla YOLO models (YOLOv8n, YOLOv8s, YOLO11n, YOLO11s). Using transfer learning with a domain-specific dataset of 581 training images, optimized input resolution ($1088 \times 1088$), and AdamW optimizer with cosine learning rate scheduling, our fine-tuned model achieves 87.54\% recall and 80.06\% precision (F1=0.836) on a test set of 60 images containing 321 vessel instances. In contrast, vanilla COCO-pretrained models achieve near 0\% recall on the same test set, with the best-performing vanilla model (YOLO11s) detecting only 10.90\% of vessels. The fine-tuned model maintains an average IoU of 0.864 for matched detections while achieving real-time inference speeds of approximately 33ms per image. These results demonstrate that domain-specific fine-tuning is essential, not optional, for maritime vessel detection applications.
}

\keywords{YOLO, Ship detection, Maritime surveillance, Transfer learning, Object detection, Bosphorus Strait, Deep learning}

\maketitle

%%=============================================================================
\section{Introduction}\label{sec:introduction}
%%=============================================================================

The Bosphorus Strait, connecting the Black Sea to the Sea of Marmara and ultimately the Mediterranean, stands as one of the world's most strategically important and congested maritime corridors. With over 40,000 vessels transiting annually, including oil tankers, cargo ships, passenger ferries, and fishing boats, the strait presents unique challenges for maritime traffic management and safety monitoring~\cite{turkishstraits2023}. The dense traffic, combined with the narrow geography of the strait (minimum width of 700 meters), residential areas along both shores, and the presence of critical infrastructure such as the Bosphorus Bridge and Fatih Sultan Mehmet Bridge, necessitates robust automated vessel detection systems.

Automated maritime surveillance serves multiple critical functions: traffic flow optimization to prevent congestion at narrow passages, collision avoidance through early vessel detection, environmental monitoring for oil spills and pollution, and support for search and rescue operations. Traditional surveillance methods relying on human operators monitoring camera feeds are limited by attention fatigue, scalability issues, and response latency. Deep learning-based object detection offers a promising solution for continuous, automated vessel monitoring.

The You Only Look Once (YOLO) family of object detectors has emerged as the state-of-the-art approach for real-time object detection, offering an attractive balance between accuracy and speed~\cite{redmon2016yolo}. Modern YOLO variants, including YOLOv8 and YOLO11 from Ultralytics~\cite{ultralytics2024yolo11}, come pretrained on the Microsoft COCO dataset~\cite{lin2014coco}, which includes a ``boat'' class. However, as this study demonstrates, these generic pretrained models fail catastrophically when applied directly to Bosphorus maritime imagery. The COCO ``boat'' class predominantly features recreational vessels, sailboats, and kayaks in varied contexts---fundamentally different from the cargo ships, tankers, and ferries that dominate Bosphorus traffic.

This paper addresses three key research questions: (1) How do vanilla COCO-pretrained YOLO models perform on Bosphorus maritime vessel detection? (2) Can domain-specific fine-tuning significantly improve detection performance? (3) What are the trade-offs between detection accuracy and inference speed for real-time maritime surveillance applications?

Our contributions are as follows:
\begin{itemize}
    \item We present the first documented evaluation of YOLO11 models for Bosphorus maritime vessel detection, demonstrating that vanilla COCO-pretrained models achieve only 1--11\% recall.
    \item We develop a fine-tuned YOLO11s model that achieves 87.54\% recall with 80.06\% precision, representing an order-of-magnitude improvement over vanilla models.
    \item We provide a comprehensive comparison of five model configurations (four vanilla, one fine-tuned) on 60 test images containing 321 vessel instances.
    \item We release our evaluation methodology and results to support reproducible maritime detection research.
\end{itemize}

The remainder of this paper is organized as follows: Sect.~\ref{sec:related} reviews related work in YOLO development and maritime object detection. Section~\ref{sec:methodology} details our methodology, including dataset characteristics, model architecture, and training configuration. Section~\ref{sec:results} presents experimental results and analysis. Section~\ref{sec:discussion} discusses findings and limitations, and Sect.~\ref{sec:conclusion} concludes with future research directions.

%%=============================================================================
\section{Related Work}\label{sec:related}
%%=============================================================================

\subsection{Evolution of YOLO Object Detectors}\label{subsec:yolo-evolution}

The YOLO (You Only Look Once) paradigm, introduced by Redmon et al.~\cite{redmon2016yolo}, revolutionized real-time object detection by framing detection as a single regression problem. Unlike two-stage detectors such as R-CNN variants that first propose regions and then classify them, YOLO processes the entire image in one forward pass, enabling real-time inference speeds.

Subsequent versions brought substantial improvements: YOLOv2~\cite{redmon2017yolo9000} introduced batch normalization and anchor boxes; YOLOv3~\cite{redmon2018yolov3} added multi-scale predictions; and YOLOv4~\cite{bochkovskiy2020yolov4} incorporated numerous architectural innovations including CSPDarknet backbone, SPP (Spatial Pyramid Pooling), and PANet path aggregation.

The Ultralytics team has continued YOLO development with YOLOv5 through YOLO11~\cite{ultralytics2024yolo11}, introducing improvements in training strategies, augmentation pipelines, and architectural refinements. YOLO11, released in 2024, features C3k2 blocks for efficient feature extraction, C2PSA (Cross Stage Partial with Spatial Attention) modules, and SPPF (Spatial Pyramid Pooling Fast) for multi-scale feature aggregation. The model family includes variants from nano (n) to extra-large (x), with the small (s) variant offering 9.4 million parameters and 21.5 GFLOPs---suitable for edge deployment scenarios common in maritime surveillance.

\subsection{Maritime Object Detection}\label{subsec:maritime-detection}

Maritime vessel detection presents unique challenges compared to general object detection. Prasad et al.~\cite{prasad2017video} identified key difficulties including variable lighting conditions (sunrise, sunset, fog), water surface reflections, vessel occlusion, and the wide range of scales from nearby ferries to distant cargo ships. Weather conditions significantly impact detection performance, with fog, rain, and waves introducing noise and reducing visibility.

Traditional maritime surveillance relied on radar and Automatic Identification System (AIS) transponders. However, AIS requires vessel cooperation and smaller vessels often lack transponders. Camera-based detection complements these systems by providing visual verification and detecting non-cooperative vessels.

Recent work has applied deep learning to maritime detection. Shao et al.~\cite{shao2018seaships} introduced the SeaShips dataset and demonstrated CNN-based detection for coastal surveillance. The Singapore Maritime Dataset~\cite{prasad2017video} provided benchmark data for near-shore vessel detection. However, these datasets capture different maritime environments than the Bosphorus, with its unique combination of urban shoreline, bridge infrastructure, and distinctive vessel types.

\subsection{Transfer Learning for Domain Adaptation}\label{subsec:transfer-learning}

Transfer learning leverages knowledge from source domains (e.g., COCO with 80 classes) to improve performance on target domains (e.g., Bosphorus maritime vessels). Pretrained weights provide effective feature representations that can be fine-tuned with limited domain-specific data~\cite{pan2010survey}.

For object detection, fine-tuning typically involves: (1) initializing with pretrained weights, (2) replacing the classification head for new classes, and (3) training on domain-specific data with a reduced learning rate. This approach requires significantly less data than training from scratch while achieving superior performance.

The effectiveness of transfer learning depends on the similarity between source and target domains. COCO's ``boat'' class provides a reasonable initialization for maritime detection, but the domain gap---recreational boats versus commercial vessels, varied backgrounds versus consistent maritime scenes---necessitates fine-tuning for optimal performance. Our work quantifies this domain gap by comparing vanilla and fine-tuned models on identical test data.

%%=============================================================================
\section{Methodology}\label{sec:methodology}
%%=============================================================================

\subsection{Dataset}\label{subsec:dataset}

We utilized the bogaz\_v\_1 dataset, version 3, sourced from Roboflow Universe~\cite{roboflow2024bogaz}. The dataset captures maritime traffic in the Bosphorus Strait with images acquired from shore-based vantage points. Table~\ref{tab:dataset} summarizes the dataset characteristics.

\begin{table}[!ht]
\centering
\caption{Dataset statistics for the bogaz\_v\_1.v3 Bosphorus maritime dataset}\label{tab:dataset}
\begin{tabular}{lc}
\toprule
\textbf{Property} & \textbf{Value} \\
\midrule
Source & Roboflow Universe \\
Classes & 1 (gemiler/ships) \\
Training images & 581 \\
Training instances & 3,321 bounding boxes \\
Background images (train) & 5 \\
Validation images & 218 \\
Validation instances & 1,217 bounding boxes \\
Background images (valid) & 2 \\
License & CC BY 4.0 \\
\bottomrule
\end{tabular}
\end{table}

The dataset contains a single class ``gemiler'' (Turkish for ``ships''), encompassing various vessel types including cargo ships, tankers, passenger ferries, fishing boats, and coast guard vessels. Annotations were provided in YOLO format with normalized bounding box coordinates. The original dataset included mixed bounding box and polygon annotations; during training, polygon annotations were automatically converted to bounding boxes.

For evaluation, we sampled 60 images using a fixed random seed (42) to ensure reproducibility. These test images contain 321 ground truth vessel instances, providing a statistically meaningful evaluation set.

\subsection{Model Architecture}\label{subsec:architecture}

We selected YOLO11s (small variant) as our base architecture for fine-tuning. The choice of the ``s'' variant over the more compact ``n'' (nano) variant was deliberate: with 9.4 million parameters compared to approximately 2.6 million for YOLO11n, the small variant provides substantially greater model capacity to capture the diversity of vessel appearances in Bosphorus imagery. This diversity encompasses multiple vessel types (cargo ships, tankers, ferries, fishing boats), a wide range of scales (from nearby ferries to distant cargo ships), and varying orientations and lighting conditions. As our experimental results confirm (Table~\ref{tab:results}), even the vanilla YOLO11s outperforms YOLO11n (10.90\% vs 2.18\% recall), suggesting that the additional capacity is beneficial for this domain. Table~\ref{tab:architecture} details the model specifications.

\begin{table}[!ht]
\centering
\caption{YOLO11s model architecture specifications}\label{tab:architecture}
\begin{tabular}{lc}
\toprule
\textbf{Property} & \textbf{Value} \\
\midrule
Total layers & 181 (100 after fusion) \\
Parameters & 9,428,179 (9.4M) \\
GFLOPs & 21.5 (21.3 fused) \\
Pretrained source & MS COCO dataset \\
Detection scales & 3 (128, 256, 512 channels) \\
\bottomrule
\end{tabular}
\end{table}

The architecture employs several key components: Conv layers with varying kernel sizes for hierarchical feature extraction, C3k2 blocks combining cross-stage partial connections with efficient $3 \times 3$ convolutions, SPPF (Spatial Pyramid Pooling Fast) for multi-scale feature aggregation, C2PSA modules incorporating spatial attention mechanisms, and a three-scale detection head for objects at different sizes. The pretrained model was initialized with COCO weights, with 493 of 499 transferable items successfully loaded.

\subsection{Training Configuration}\label{subsec:training}

Table~\ref{tab:training} presents the complete training configuration. We used Google Colab with an NVIDIA L4 GPU (22.7 GB memory) for training.

\begin{table}[!ht]
\centering
\caption{Training hyperparameters and configuration}\label{tab:training}
\begin{tabular}{lcc}
\toprule
\textbf{Parameter} & \textbf{Value} & \textbf{Notes} \\
\midrule
Hardware & NVIDIA L4 & 22.7 GB VRAM \\
Input resolution & $1088 \times 1088$ & Optimized for 1080p source \\
Batch size & 17 & Auto-computed at 58\% GPU \\
Epochs & 50 & All completed \\
Optimizer & AdamW & momentum=0.937 \\
Learning rate & lr0=0.001, lrf=0.01 & Cosine decay to lr0$\times$lrf \\
Weight decay & 0.0005 & L2 regularization \\
Warmup epochs & 3 & warmup\_bias\_lr=0.1 \\
Early stopping & patience=10 & Not triggered \\
Mixed precision & Enabled & AMP training \\
Mosaic augmentation & Until epoch 40 & close\_mosaic=10 \\
Image caching & RAM & 1.1 GB train, 0.4 GB valid \\
\bottomrule
\end{tabular}
\end{table}

The input resolution of $1088 \times 1088$ was carefully selected based on both architectural constraints and source imagery characteristics. YOLO architectures require input dimensions divisible by 32 due to five successive downsampling stages with stride 2 in the backbone network ($2^5 = 32$). Our source imagery is $1920 \times 1080$ (Full HD), and 1080 is not divisible by 32. The nearest compliant value is 1088 ($1088 = 34 \times 32$), which minimizes aspect ratio distortion compared to the default $640 \times 640$ while preserving substantially more spatial detail. This resolution represents $2.89\times$ more pixels than the standard $640 \times 640$ YOLO input, significantly improving detection of small and distant vessels at the cost of modestly increased computation.

Data augmentation included mosaic augmentation (combining four training images), random horizontal flipping (50\% probability), HSV color space adjustments (hue$\pm$0.015, saturation$\pm$0.7, value$\pm$0.4), and albumentations transforms (blur, median blur, grayscale conversion, CLAHE). Mosaic augmentation was disabled for the final 10 epochs to fine-tune on full-resolution images.

Training completed in 0.207 hours (12.4 minutes) for 50 epochs. The model achieved best validation mAP50 of 0.840 and mAP50-95 of 0.627, with final validation precision of 0.788 and recall of 0.784. Note that validation metrics use Ultralytics' mAP-based evaluation with multiple IoU thresholds, while our test evaluation (Sect.~\ref{sec:results}) uses fixed IoU=0.5 matching, which accounts for slight differences in reported recall values.

\subsection{Evaluation Framework}\label{subsec:evaluation}

We compared five model configurations as detailed in Table~\ref{tab:models}. Vanilla models used default Ultralytics parameters to represent ``out-of-box'' performance that a practitioner would obtain without domain expertise. The fine-tuned model used its trained optimal parameters.

\begin{table}[!ht]
\centering
\caption{Model configurations for comparative evaluation}\label{tab:models}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Weights} & \textbf{Input Size} & \textbf{Classes} \\
\midrule
YOLOv8n (vanilla) & yolov8n.pt & $640 \times 640$ & 80 (COCO) \\
YOLOv8s (vanilla) & yolov8s.pt & $640 \times 640$ & 80 (COCO) \\
YOLO11n (vanilla) & yolo11n.pt & $640 \times 640$ & 80 (COCO) \\
YOLO11s (vanilla) & yolo11s.pt & $640 \times 640$ & 80 (COCO) \\
YOLO11s (fine-tuned) & best.pt & $1088 \times 1088$ & 1 (ships) \\
\bottomrule
\end{tabular}
\end{table}

All models used a confidence threshold of 0.25. For vanilla models, we filtered predictions to the ``boat'' class (COCO class 8). Detection matching used IoU threshold of 0.5 (standard COCO criterion) with greedy assignment: each ground truth box was matched to the highest-IoU prediction exceeding the threshold, with each prediction matching at most one ground truth.

Evaluation metrics included: precision (TP / (TP + FP)), recall (TP / (TP + FN)), F1 score (harmonic mean of precision and recall), average IoU of matched detections, and inference time per image.

%%=============================================================================
\section{Experimental Results}\label{sec:results}
%%=============================================================================

\subsection{Overall Performance Comparison}\label{subsec:overall-performance}

Table~\ref{tab:results} presents the primary experimental results across all five model configurations on the 60-image test set containing 321 ground truth vessel instances.

\begin{table}[!ht]
\centering
\caption{Comparative detection performance on 60 test images (321 ground truth vessels)}\label{tab:results}
\begin{tabular}{lrrrrrrr}
\toprule
\textbf{Model} & \textbf{Det.} & \textbf{TP} & \textbf{FP} & \textbf{FN} & \textbf{Prec.} & \textbf{Recall} & \textbf{F1} \\
\midrule
YOLOv8n & 21 & 4 & 17 & 317 & 19.05\% & 1.25\% & 2.34\% \\
YOLOv8s & 40 & 21 & 19 & 300 & 52.50\% & 6.54\% & 11.63\% \\
YOLO11n & 28 & 7 & 21 & 314 & 25.00\% & 2.18\% & 4.01\% \\
YOLO11s & 75 & 35 & 40 & 286 & 46.67\% & 10.90\% & 17.68\% \\
\textbf{Fine-tuned} & \textbf{351} & \textbf{281} & \textbf{70} & \textbf{40} & \textbf{80.06\%} & \textbf{87.54\%} & \textbf{83.63\%} \\
\bottomrule
\end{tabular}
\end{table}

The results reveal a dramatic performance gap between vanilla and fine-tuned models. The fine-tuned YOLO11s achieves 87.54\% recall, detecting 281 of 321 vessels, while the best-performing vanilla model (YOLO11s) detects only 35 vessels (10.90\% recall). This represents an $8.0\times$ improvement in recall through domain-specific fine-tuning.

Precision follows a similar pattern: the fine-tuned model achieves 80.06\% precision compared to the vanilla models' range of 19--53\%. The F1 score of 83.63\% for the fine-tuned model demonstrates balanced precision-recall performance, compared to the best vanilla F1 of only 17.68\%.

\subsection{Detection Quality Analysis}\label{subsec:quality-analysis}

Table~\ref{tab:iou_time} presents localization accuracy (IoU) and inference speed metrics.

\begin{table}[!ht]
\centering
\caption{Localization accuracy and inference speed comparison}\label{tab:iou_time}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Avg. IoU (matched)} & \textbf{Avg. Time (ms)} \\
\midrule
YOLOv8n & 0.74 & 30.7 \\
YOLOv8s & 0.74 & 30.7 \\
YOLO11n & 0.75 & 31.1 \\
YOLO11s & 0.78 & 32.1 \\
Fine-tuned & \textbf{0.864} & 33.3 \\
\bottomrule
\end{tabular}
\end{table}

The fine-tuned model achieves superior bounding box localization with an average IoU of 0.864 for matched detections, compared to 0.74--0.78 for vanilla models. This indicates that fine-tuning improves not only detection rate but also localization accuracy.

Inference speed remains competitive despite the larger input resolution. The fine-tuned model processes images in approximately 33ms ($1088 \times 1088$) compared to 31ms for vanilla models ($640 \times 640$). The $2.89\times$ increase in pixel count results in only a modest 7\% increase in inference time, demonstrating the efficiency of the YOLO11 architecture. Both configurations support real-time processing at 30+ FPS.

\subsection{Per-Image Analysis}\label{subsec:per-image}

Analysis of individual image results reveals consistent patterns. Figure~\ref{fig:comparison} illustrates the detection performance across representative test images.

\begin{figure}[!ht]
\centering
\includegraphics[width=\textwidth]{comparison_grid.png}
\caption{Detection comparison across representative test images. Each row shows a different test image with detections from all five models. Green boxes indicate true positives, red boxes indicate false positives. The fine-tuned model consistently detects vessels that vanilla models miss entirely.}\label{fig:comparison}
\end{figure}

Several patterns emerge from per-image analysis:

\textbf{Complete Vanilla Failure:} On 42 of 60 images (70\%), at least one vanilla model produced zero detections. On 18 images (30\%), all four vanilla models failed to detect any vessels despite ground truth counts of 3--9 ships per image.

\textbf{Fine-tuned Robustness:} The fine-tuned model detected at least one vessel in 59 of 60 images (98.3\%), with complete detection (100\% recall) on 15 images.

\textbf{Case Study---Complete Vanilla Failure:} Image \texttt{20250126\_151011\_mp4-0093} contains 8 ground truth vessels. All four vanilla models produced zero detections. The fine-tuned model detected all 8 vessels with 100\% precision and an average IoU of 0.877.

\textbf{Case Study---Dense Scene:} Image \texttt{20250126\_151829} contains 20 ground truth vessels in a crowded maritime scene. Vanilla models detected 0--3 vessels (0--15\% recall). The fine-tuned model detected 12 vessels (60\% recall, 100\% precision), demonstrating capability in challenging dense scenarios while acknowledging room for improvement.

\textbf{Case Study---False Positive Analysis:} The fine-tuned model's 70 false positives arise from several identifiable sources in dense maritime scenes. \textit{Shore structures} constitute the dominant source of false positives: buildings, piers, and docks along the Bosphorus shoreline present rectangular shapes and high-contrast edges that resemble vessel superstructures. \textit{Overlapping vessels} in crowded scenes create ambiguous boundaries, causing the detector to segment partially visible vessels as separate objects. \textit{Large vessels spanning multiple scales} may trigger multiple detections when different parts of the ship (bow, bridge, stern) each match learned vessel features. \textit{Wake foam and water disturbances} present high-contrast white patterns against the dark water surface, occasionally mimicking small vessel signatures. In safety-critical maritime surveillance applications, such over-detection is generally preferable to missed vessels, as human operators can quickly dismiss false alarms while missed vessels pose genuine safety risks.

\subsection{Performance Visualization}\label{subsec:visualization}

Figure~\ref{fig:charts} presents performance metrics in chart form for visual comparison across models.

\begin{figure}[!ht]
\centering
\includegraphics[width=\textwidth]{comparison_charts.png}
\caption{Performance comparison charts showing (left) precision, recall, and F1 scores, and (right) detection counts versus ground truth across all models.}\label{fig:charts}
\end{figure}

The visualization emphasizes the order-of-magnitude gap between vanilla and fine-tuned performance. While vanilla models cluster near zero recall, the fine-tuned model approaches the ground truth detection target.

%%=============================================================================
\section{Discussion}\label{sec:discussion}
%%=============================================================================

\subsection{Why Fine-tuning Helps Dramatically}\label{subsec:why-finetuning}

The substantial performance gap between vanilla and fine-tuned models can be attributed to several factors:

\textbf{Domain Shift:} The COCO ``boat'' class predominantly features recreational vessels (sailboats, kayaks, motorboats) in diverse contexts (lakes, rivers, marinas). Bosphorus traffic comprises commercial vessels (cargo ships, tankers, ferries) against consistent maritime backgrounds. This semantic gap prevents effective knowledge transfer without fine-tuning.

\textbf{Background Adaptation:} The fine-tuned model learns Bosphorus-specific background patterns: characteristic water textures, shoreline buildings, bridge structures (Bosphorus Bridge, Fatih Sultan Mehmet Bridge), and typical sky conditions. Vanilla models, unfamiliar with these backgrounds, generate both false negatives (missing vessels) and false positives (misclassifying background elements).

\textbf{Scale Adaptation:} Maritime vessels in the Bosphorus appear at various scales depending on distance (20m to 2km from camera). The higher input resolution ($1088 \times 1088$) preserves details of distant vessels that would be lost at $640 \times 640$. Fine-tuning at this resolution teaches the model to detect vessels across the full scale range.

\textbf{Single-Class Focus:} The fine-tuned model is optimized for a single class (ships) rather than 80 COCO classes. This specialization allows the model to dedicate full capacity to distinguishing vessels from maritime backgrounds without competition from irrelevant classes.

\subsection{Input Resolution Analysis}\label{subsec:resolution-analysis}

The choice of $1088 \times 1088$ input resolution represents a deliberate trade-off. With $2.89\times$ more pixels than the standard $640 \times 640$, this resolution:

\begin{itemize}
    \item Preserves details of distant/small vessels that would otherwise be lost
    \item Maintains aspect ratio compatibility with $1920 \times 1080$ source imagery
    \item Incurs only modest speed penalty (approximately 7\% slower, 33.3ms vs 31ms) due to efficient YOLO architecture
    \item Enables detection of vessels at greater distances, critical for early warning in traffic management
\end{itemize}

The minimal speed impact demonstrates that input resolution can be increased substantially for specialized applications without sacrificing real-time capability.

\subsection{Comparison Fairness}\label{subsec:fairness}

Our experimental design intentionally compares vanilla models at default parameters against the fine-tuned model at optimal parameters. This reflects the practical question: ``What performance difference would a maritime operator observe between downloading a pretrained model versus investing in domain-specific fine-tuning?''

We acknowledge that vanilla model performance could potentially improve with hyperparameter optimization (e.g., lower confidence thresholds, test-time augmentation). However, such optimization without domain-specific training data cannot overcome the fundamental distribution shift between COCO boats and Bosphorus vessels. The near-zero recall of vanilla models indicates that the pretrained features do not adequately represent Bosphorus maritime vessels.

\subsection{Limitations}\label{subsec:limitations}

Several limitations constrain the generalizability of our findings:

\textbf{Test Set Size:} While 60 images with 321 vessel instances provide statistically meaningful results, a larger test set would strengthen conclusions about edge cases and failure modes.

\textbf{Single Class:} Our model detects ``ships'' as a unified category without distinguishing vessel types (cargo, tanker, ferry, fishing). Multi-class detection would provide more actionable information for traffic management.

\textbf{Daytime Conditions:} The dataset and evaluation focus on daytime imagery. Night-time, low-light, and thermal imaging performance remain untested.

\textbf{Weather Variability:} Test images represent predominantly clear conditions. Performance under fog, rain, and rough sea states requires further evaluation.

\textbf{Geographic Specificity:} The model is trained and evaluated exclusively on Bosphorus imagery. Generalization to other waterways (Dardanelles, Suez Canal, other straits) is unknown.

\textbf{Static Images:} Evaluation used static images rather than video streams. Real-time deployment considerations including tracking, temporal consistency, and stream latency were not assessed.

%%=============================================================================
\section{Conclusion}\label{sec:conclusion}
%%=============================================================================

This paper demonstrates that domain-specific fine-tuning is essential for maritime vessel detection in the Bosphorus Strait. Our comprehensive evaluation of five YOLO model configurations reveals that vanilla COCO-pretrained models fail catastrophically on Bosphorus maritime imagery, achieving only 1--11\% recall despite including a ``boat'' class. In contrast, our fine-tuned YOLO11s model achieves 87.54\% recall with 80.06\% precision (F1=0.836), representing an order-of-magnitude improvement.

Key findings include:
\begin{itemize}
    \item Vanilla YOLO models (YOLOv8n/s, YOLO11n/s) detect 1--11\% of Bosphorus vessels, demonstrating severe domain shift from COCO training data.
    \item Fine-tuning with 581 domain-specific images for 50 epochs (12.4 minutes on NVIDIA L4) transforms unusable models into production-ready solutions.
    \item Higher input resolution ($1088 \times 1088$) improves small vessel detection with minimal speed penalty, maintaining real-time capability at 30+ FPS.
    \item The fine-tuned model achieves superior localization (0.864 average IoU) in addition to higher detection rates.
\end{itemize}

The practical implication is clear: practitioners deploying vessel detection for maritime surveillance must invest in domain-specific fine-tuning. Off-the-shelf pretrained models, despite including boat classes, cannot reliably detect vessels in specialized maritime environments.

Future work will extend this research in several directions: multi-class vessel classification (cargo, tanker, ferry, fishing), night-time and thermal imaging adaptation, video-based tracking with temporal consistency, edge deployment optimization for embedded systems, and evaluation of generalization to other maritime corridors. Additionally, systematic ablation studies would strengthen understanding of individual component contributions---particularly confidence threshold ablation to characterize the precision-recall trade-off curve, and input resolution ablation to quantify detection gains at intermediate resolutions (e.g., 640, 832, 1088) against computational cost. Integration with AIS data for vessel identification verification presents an additional avenue for comprehensive maritime surveillance systems.

%%=============================================================================
%% Acknowledgements
%%=============================================================================

\begin{acknowledgements}
The authors thank \"{U}sk\"{u}dar University for institutional support, Roboflow for dataset hosting and annotation tools, Google Colab for GPU compute resources (NVIDIA L4), and Ultralytics for the YOLO implementation.
\end{acknowledgements}

%%=============================================================================
%% Declarations
%%=============================================================================

\begin{declarations}

\subsection*{Funding}
No funding was received for conducting this study.

\subsection*{Conflict of interest}
The authors declare that they have no conflict of interest.

\subsection*{Ethics approval}
Not applicable. This research does not involve human participants, their data, or biological material.

\subsection*{Consent to participate}
Not applicable.

\subsection*{Consent for publication}
Not applicable.

\subsection*{Data availability}
The bogaz\_v\_1 dataset used in this study is publicly available at Roboflow Universe (\url{https://universe.roboflow.com/bosphorus-vision-project/bogaz_v_1-o9ldr}) under CC BY 4.0 license.

\subsection*{Code availability}
The code and trained model weights are available at \url{https://github.com/Mizar-107/Uskudar-University-Thesis-Repository}.

\subsection*{Author contributions}
\textbf{Recep Ertu\u{g}rul Ek\c{s}i:} Conceptualization, Methodology, Software, Investigation, Writing -- Original Draft, Visualization. \textbf{Rowanda D. Ahmed:} Supervision, Writing -- Review \& Editing, Validation.

\end{declarations}

%%=============================================================================
%% References
%%=============================================================================

\bibliographystyle{sn-basic}
\bibliography{bibliography}

\end{document}
